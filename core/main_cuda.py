"""
CUDA-accelerated Genetic Algorithm Trading Agent Training
Supports NVIDIA GPU CUDA acceleration
"""

import time
from pathlib import Path
import json
import torch
import numpy as np
import sys
import os
from numpy._core.multiarray import _reconstruct as numpy_reconstruct
from numpy import ndarray as numpy_ndarray

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from cuda_gpu_utils import CudaGPUManager, get_cuda_gpu_manager, check_cuda_compatibility, optimize_cuda_settings
from cuda_accelerated_ga import CudaGPUAcceleratedGA, CudaGAConfig
from data_processor import GPUDataProcessor

# æ€§èƒ½åˆ†æ
try:
    from performance_profiler import get_profiler, start_monitoring, stop_monitoring, print_summary, save_report, timer
    PERFORMANCE_PROFILER_AVAILABLE = True
    print("ğŸ” æ€§èƒ½åˆ†æå™¨å·²å¯ç”¨")
except ImportError:
    PERFORMANCE_PROFILER_AVAILABLE = False
    print("âš ï¸  æ€§èƒ½åˆ†æå™¨ä¸å¯ç”¨")
    # åˆ›å»ºç©ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    class timer:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
    def start_monitoring(*args, **kwargs):
        pass
    def stop_monitoring():
        pass
    def print_summary(*args, **kwargs):
        pass
    def save_report(*args, **kwargs):
        pass

# ç¡®ä¿resultsç›®å½•å­˜åœ¨
results_dir = Path('../results')
results_dir.mkdir(exist_ok=True)


def main():
    """Main function - CUDA version integrated configuration and automated workflow"""
    # Allowlist numpy._core.multiarray._reconstruct and numpy.ndarray for torch.load with weights_only=True
    torch.serialization.add_safe_globals([numpy_reconstruct, numpy_ndarray])

    # ==============================================================================
    # ======================= åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„è®­ç»ƒå‚æ•° ============================
    # ==============================================================================
    TRAINING_CONFIG = {
        # ==================== æ ¸å¿ƒè®­ç»ƒå‚æ•° ====================
        
        # --- æ•°æ®é…ç½® ---
        "data_directory": "../data",     # æ•°æ®æ–‡ä»¶ç›®å½•
        "window_size": 350,             # ç‰¹å¾å·¥ç¨‹çª—å£å¤§å°
        "normalization": "rolling",     # å½’ä¸€åŒ–æ–¹æ³•: 'rolling', 'minmax_local', 'hybrid'
        "batch_size": 1000,             # CUDAä¸Šå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        
        # --- é—ä¼ ç®—æ³•å‚æ•° ---
        "population_size": 5000,         # ç§ç¾¤å¤§å° (CUDAä¸Šæ¨è: 1000-5000)
        "generations": -1,              # è®­ç»ƒä»£æ•° (-1=æ— é™è®­ç»ƒ, æ¨è: 100-1000)
        "mutation_rate": 0.01,           # å˜å¼‚ç‡ (æ¨è: 0.005-0.02)
        "crossover_rate": 0.8,           # äº¤å‰ç‡ (æ¨è: 0.7-0.9)
        "elite_ratio": 0.05,              # ç²¾è‹±ä¿ç•™æ¯”ä¾‹ (æ¨è: 0.05-0.15)
        "early_stop_patience": 100,      # æ— æ”¹è¿›åœæ­¢ä»£æ•° (æ¨è: 50-200)
        "use_torch_scan": True,          # ä½¿ç”¨torch.scanä¼˜åŒ–å›æµ‹ (æ¨è: True)
        
        # --- äº¤æ˜“ç­–ç•¥å‚æ•° (ç°åœ¨ç”±é—ä¼ ç®—æ³•è‡ªåŠ¨è¿›åŒ–) ---
        # æ³¨æ„ï¼šä»¥ä¸‹å‚æ•°ç°åœ¨ä½œä¸ºåŸºå› è‡ªåŠ¨è¿›åŒ–ï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®
        # - ä¹°å…¥é˜ˆå€¼: è‡ªåŠ¨åœ¨ [0.55, 0.8] èŒƒå›´å†…è¿›åŒ–
        # - å–å‡ºé˜ˆå€¼: è‡ªåŠ¨åœ¨ [0.2, 0.45] èŒƒå›´å†…è¿›åŒ–
        # - æ­¢æŸæ¯”ä¾‹: è‡ªåŠ¨åœ¨ [0.02, 0.08] èŒƒå›´å†…è¿›åŒ–  
        # - æœ€å¤§ä»“ä½: è‡ªåŠ¨åœ¨ [0.5, 1.0] èŒƒå›´å†…è¿›åŒ–
        # - æœ€å¤§å›æ’¤: è‡ªåŠ¨åœ¨ [0.1, 0.25] èŒƒå›´å†…è¿›åŒ–
        
        # --- é€‚åº”åº¦æƒé‡ (æ€»å’Œåº”ä¸º1.0) ---
        "sharpe_weight": 0.4,            # å¤æ™®æ¯”ç‡æƒé‡
        "drawdown_weight": 0.2,          # å›æ’¤æƒ©ç½šæƒé‡
        "stability_weight": 0.4,         # äº¤æ˜“ç¨³å®šæ€§æƒé‡
        
        # ==================== CUDAä¸“ç”¨é…ç½® ====================
        
        # --- GPUè®¾ç½® ---
        "gpu_device_id": 0,              # GPUè®¾å¤‡ID (0ä¸ºç¬¬ä¸€ä¸ªGPU)
        "gpu_memory_fraction": 0.9,      # GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹ (0.0-1.0)
        "mixed_precision": False,        # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (å®éªŒæ€§)
        
        # ==================== ç³»ç»Ÿé…ç½® ====================
        
        # --- ä¿å­˜è®¾ç½® ---
        "results_dir": "../results",     # ç»“æœè¾“å‡ºç›®å½•
        "save_checkpoints": True,        # æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
        "checkpoint_interval": 1,      # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš” (CUDAä¸Šå¯ä»¥æ›´é•¿)
        "auto_save_best": True,          # æ˜¯å¦è‡ªåŠ¨ä¿å­˜æœ€ä½³ä¸ªä½“
        "save_best_interval": 100,       # æ¯éš”å¤šå°‘ä»£ä¿å­˜æœ€ä¼˜ä¸ªä½“
        
        # --- æ—¥å¿—è®¾ç½® ---
        "save_generation_results": True, # æ˜¯å¦ä¿å­˜æ¯ä»£ç»“æœ
        "generation_log_interval": 1,    # æ—¥å¿—è®°å½•é—´éš”
    }
    
    # ==============================================================================
    # ======================== é¢„è®¾é…ç½®æ¨¡æ¿ (å¯é€‰æ‹©ä½¿ç”¨) =========================
    # ==============================================================================
    
    # ğŸš€ å¿«é€Ÿæµ‹è¯•é…ç½® (CUDAç‰ˆ)
    QUICK_TEST_CONFIG = {
        **TRAINING_CONFIG,
        "population_size": 200,
        "generations": 20,
        "checkpoint_interval": 10,
        "batch_size": 500,
    }
    
    # ğŸ’ª é«˜æ€§èƒ½é…ç½® (é€‚åˆé«˜ç«¯NVIDIA GPU)
    HIGH_PERFORMANCE_CONFIG = {
        **TRAINING_CONFIG,
        "population_size": 3000,
        "generations": 500,
        "checkpoint_interval": 50,
        "batch_size": 2000,
        "early_stop_patience": 150,
    }
    
    # ğŸ”¥ æé™æ€§èƒ½é…ç½® (RTX 4090/A100ç­‰)
    EXTREME_PERFORMANCE_CONFIG = {
        **TRAINING_CONFIG,
        "population_size": 5000,
        "generations": 1000,
        "checkpoint_interval": 25,
        "batch_size": 3000,
        "early_stop_patience": 200,
        "gpu_memory_fraction": 0.95,
    }
    
    # ğŸ›¡ï¸ ä¿å®ˆäº¤æ˜“ç­–ç•¥ (CUDAç‰ˆ) - æ³¨é‡é£é™©æ§åˆ¶
    CONSERVATIVE_CONFIG = {
        **TRAINING_CONFIG,
        # æ³¨æ„ï¼šäº¤æ˜“å‚æ•°ç°åœ¨ç”±åŸºå› è‡ªåŠ¨è¿›åŒ–ï¼Œè¿™é‡Œåªè°ƒæ•´é€‚åº”åº¦æƒé‡æ¥åå‘ä¿å®ˆç­–ç•¥
        "sharpe_weight": 0.6,            # æ›´é‡è§†é£é™©è°ƒæ•´æ”¶ç›Š
        "drawdown_weight": 0.4,          # æ›´é‡è§†å›æ’¤æ§åˆ¶
        "stability_weight": 0.0,         # ä¸è€ƒè™‘äº¤æ˜“é¢‘ç‡
        "population_size": 1500,         # æ›´å¤§çš„ç§ç¾¤ä»¥æé«˜ç¨³å®šæ€§
    }
    
    # âš¡ æ¿€è¿›äº¤æ˜“ç­–ç•¥ (CUDAç‰ˆ) - è¿½æ±‚é«˜æ”¶ç›Š
    AGGRESSIVE_CONFIG = {
        **TRAINING_CONFIG,
        # æ³¨æ„ï¼šäº¤æ˜“å‚æ•°ç°åœ¨ç”±åŸºå› è‡ªåŠ¨è¿›åŒ–ï¼Œè¿™é‡Œåªè°ƒæ•´é€‚åº”åº¦æƒé‡æ¥åå‘æ¿€è¿›ç­–ç•¥
        "sharpe_weight": 0.3,            # ç›¸å¯¹è¾ƒå°‘é‡è§†é£é™©è°ƒæ•´
        "drawdown_weight": 0.2,          # è¾ƒå°‘é‡è§†å›æ’¤æ§åˆ¶
        "stability_weight": 0.5,         # é‡è§†äº¤æ˜“é¢‘ç‡å’Œæ´»è·ƒåº¦
        "population_size": 2000,         # æ›´å¤§çš„ç§ç¾¤ä»¥æ¢ç´¢æ›´å¤šç­–ç•¥
    }
    
    # ğŸ”„ é•¿æœŸè®­ç»ƒé…ç½® (CUDAç‰ˆ)
    LONG_TERM_CONFIG = {
        **TRAINING_CONFIG,
        "generations": -1,               # æ— é™è®­ç»ƒ
        "early_stop_patience": 200,      # æ›´é•¿çš„è€å¿ƒ
        "checkpoint_interval": 100,      # æ›´é•¿çš„ä¿å­˜é—´éš”
        "population_size": 2000,         # æ›´å¤§çš„ç§ç¾¤
    }
    
    # ğŸ§ª å®éªŒæ€§é…ç½® (ä½¿ç”¨æœ€æ–°CUDAç‰¹æ€§)
    EXPERIMENTAL_CONFIG = {
        **TRAINING_CONFIG,
        "mixed_precision": True,         # æ··åˆç²¾åº¦è®­ç»ƒ
        "use_torch_scan": True,          # ä½¿ç”¨æœ€æ–°çš„torch.scan
        "population_size": 4000,
        "batch_size": 2500,
        "gpu_memory_fraction": 0.95,
    }
    
    # ==============================================================================
    # =================== é€‰æ‹©è¦ä½¿ç”¨çš„é…ç½® (ä¿®æ”¹è¿™é‡Œ) ===========================
    # ==============================================================================
    
    # é€‰æ‹©é…ç½® (å–æ¶ˆæ³¨é‡Šæƒ³è¦ä½¿ç”¨çš„é…ç½®)
    ACTIVE_CONFIG = TRAINING_CONFIG              # é»˜è®¤é…ç½®
    # ACTIVE_CONFIG = QUICK_TEST_CONFIG          # å¿«é€Ÿæµ‹è¯•
    # ACTIVE_CONFIG = HIGH_PERFORMANCE_CONFIG    # é«˜æ€§èƒ½
    # ACTIVE_CONFIG = EXTREME_PERFORMANCE_CONFIG # æé™æ€§èƒ½
    # ACTIVE_CONFIG = CONSERVATIVE_CONFIG        # ä¿å®ˆç­–ç•¥
    # ACTIVE_CONFIG = AGGRESSIVE_CONFIG          # æ¿€è¿›ç­–ç•¥
    # ACTIVE_CONFIG = LONG_TERM_CONFIG           # é•¿æœŸè®­ç»ƒ
    # ACTIVE_CONFIG = EXPERIMENTAL_CONFIG        # å®éªŒæ€§é…ç½®
    
    # ==============================================================================
    # ======================= å‚æ•°ä¿®æ”¹åŒºåŸŸç»“æŸ ==================================
    # ==============================================================================

    print("=== CUDA GPUåŠ é€Ÿé—ä¼ ç®—æ³•äº¤æ˜“å‘˜è®­ç»ƒå¼€å§‹ ===")
    
    # --- 1. CUDAç¯å¢ƒæ£€æŸ¥ä¸ä¼˜åŒ– ---
    print("\n--- CUDAç¯å¢ƒæ£€æŸ¥ ---")
    cuda_info = check_cuda_compatibility()
    for key, value in cuda_info.items():
        if key == 'gpus':
            print(f"å¯ç”¨GPU:")
            for i, gpu in enumerate(value):
                print(f"  GPU {i}: {gpu['name']} ({gpu['memory_gb']:.1f}GB)")
        elif key not in ['gpus']:
            print(f"{key}: {value}")
    
    if not cuda_info['cuda_available']:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥CUDAå®‰è£…")
        return
    
    print("âœ… CUDAç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    
    # ä¼˜åŒ–CUDAè®¾ç½®
    optimize_cuda_settings()
    
    # --- 2. è‡ªåŠ¨åŒ–è®¾ç½®ä¸è·¯å¾„ç®¡ç† ---
    output_dir = Path(ACTIVE_CONFIG["results_dir"])
    checkpoint_dir = output_dir / "checkpoints"
    data_dir = Path(ACTIVE_CONFIG["data_directory"])
    
    output_dir.mkdir(exist_ok=True)
    checkpoint_dir.mkdir(exist_ok=True)

    print("\n--- è®­ç»ƒå‚æ•° ---")
    for key, value in ACTIVE_CONFIG.items():
        print(f"{key}: {value}")
    print("--------------------\n")
    
    # --- 3. è‡ªåŠ¨å‘ç°æœ€æ–°çš„æ•°æ®æ–‡ä»¶ ---
    try:
        data_files = sorted(data_dir.glob("*.csv"), key=os.path.getmtime, reverse=True)
        if not data_files:
            print(f"æ•°æ®ç›®å½• '{data_dir}' ä¸­æœªæ‰¾åˆ°ä»»ä½•.csvæ–‡ä»¶ã€‚")
            return
        latest_data_file = data_files[0]
        print(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„æ•°æ®æ–‡ä»¶: {latest_data_file}")
    except FileNotFoundError:
        print(f"æ•°æ®ç›®å½• '{data_dir}' ä¸å­˜åœ¨ã€‚")
        return

    # --- 4. è‡ªåŠ¨å‘ç°æœ€æ–°çš„æ£€æŸ¥ç‚¹ ---
    load_checkpoint_path = None
    if ACTIVE_CONFIG["save_checkpoints"]:
        checkpoints = sorted(checkpoint_dir.glob("*.pt"), key=os.path.getmtime, reverse=True)
        if checkpoints:
            latest_checkpoint = checkpoints[0]
            print(f"å‘ç°æœ€æ–°çš„æ£€æŸ¥ç‚¹: {latest_checkpoint}")
            
            # æ£€æŸ¥æ£€æŸ¥ç‚¹ä¸­çš„å‚æ•°æ˜¯å¦ä¸å½“å‰é…ç½®åŒ¹é…
            try:
                ckpt = torch.load(latest_checkpoint, map_location='cpu')
                if ckpt['config'].population_size != ACTIVE_CONFIG['population_size']:
                    print(f"è­¦å‘Š: æ£€æŸ¥ç‚¹ä¸­çš„ç§ç¾¤å¤§å° ({ckpt['config'].population_size}) ä¸å½“å‰é…ç½® ({ACTIVE_CONFIG['population_size']}) ä¸åŒ¹é…ã€‚")
                    print("å°†å¿½ç•¥æ£€æŸ¥ç‚¹ï¼Œå¼€å§‹æ–°çš„è®­ç»ƒã€‚")
                else:
                    load_checkpoint_path = latest_checkpoint
                    print(f"æ£€æŸ¥ç‚¹å‚æ•°åŒ¹é…ï¼Œå°†ä» '{load_checkpoint_path}' ç»§ç»­è®­ç»ƒã€‚")
            except Exception as e:
                print(f"æ— æ³•åŠ è½½æˆ–è§£ææ£€æŸ¥ç‚¹ '{latest_checkpoint}': {e}")
                print("å°†å¿½ç•¥æ£€æŸ¥ç‚¹ï¼Œå¼€å§‹æ–°çš„è®­ç»ƒã€‚")
        else:
            print("æœªå‘ç°æ£€æŸ¥ç‚¹ï¼Œå°†å¼€å§‹æ–°çš„è®­ç»ƒã€‚")

    try:
        # --- 5. å¯åŠ¨æ€§èƒ½ç›‘æ§ ---
        if PERFORMANCE_PROFILER_AVAILABLE:
            start_monitoring(interval=2.0)  # æ¯2ç§’è®°å½•ä¸€æ¬¡å†…å­˜ä½¿ç”¨
            print("ğŸ” æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
        
        # --- 6. åˆå§‹åŒ–CUDA GPUç®¡ç†å™¨ ---
        with timer("gpu_initialization", "setup"):
            print("åˆå§‹åŒ–CUDA GPUç¯å¢ƒ...")
            gpu_manager = get_cuda_gpu_manager(device_id=ACTIVE_CONFIG.get("gpu_device_id", 0))
        
        # è®¾ç½®GPUå†…å­˜ä½¿ç”¨é™åˆ¶
        if "gpu_memory_fraction" in ACTIVE_CONFIG:
            gpu_manager.set_memory_fraction(ACTIVE_CONFIG["gpu_memory_fraction"])
        
        print(f"âœ… CUDA GPUåŠ é€Ÿå·²{'å¯ç”¨' if gpu_manager.device.type == 'cuda' else 'ç¦ç”¨'}")
        
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        gpu_alloc, gpu_total, sys_used, sys_total = gpu_manager.get_memory_usage()
        print(f"GPUå†…å­˜: {gpu_alloc:.2f}GB / {gpu_total:.2f}GB")
        print(f"ç³»ç»Ÿå†…å­˜: {sys_used:.2f}GB / {sys_total:.2f}GB")

        # --- 7. æ•°æ®å¤„ç† ---
        with timer("data_processing", "setup"):
            print("\nå¼€å§‹æ•°æ®å¤„ç†...")
            
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹GPUDataProcessorä»¥æ”¯æŒCUDA
            # æš‚æ—¶ä½¿ç”¨åŸæœ‰çš„å¤„ç†å™¨ï¼Œä½†éœ€è¦ç¡®ä¿æ•°æ®èƒ½æ­£ç¡®è½¬ç§»åˆ°CUDA GPU
            processor = GPUDataProcessor(
                window_size=ACTIVE_CONFIG["window_size"],
                normalization_method=ACTIVE_CONFIG["normalization"],
                gpu_manager=gpu_manager  # ä¼ å…¥CUDA GPUç®¡ç†å™¨
            )
            
            train_features, train_labels = processor.load_and_process_data(latest_data_file)
            print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_features.shape}, æ ‡ç­¾æ•°æ®å½¢çŠ¶: {train_labels.shape}")

        # --- 8. é…ç½®å¹¶åˆå§‹åŒ–CUDAé—ä¼ ç®—æ³• ---
        with timer("ga_initialization", "setup"):
            ga_config = CudaGAConfig(
                population_size=ACTIVE_CONFIG["population_size"],
                max_generations=ACTIVE_CONFIG["generations"],
                mutation_rate=ACTIVE_CONFIG["mutation_rate"],
                crossover_rate=ACTIVE_CONFIG["crossover_rate"],
                elite_ratio=ACTIVE_CONFIG["elite_ratio"],
                feature_dim=train_features.shape[1],
                # æ³¨æ„ï¼šäº¤æ˜“ç­–ç•¥å’Œé£é™©ç®¡ç†å‚æ•°ç°åœ¨ä½œä¸ºåŸºå› è‡ªåŠ¨è¿›åŒ–ï¼Œä¸å†ä»é…ç½®ä¸­è¯»å–
                # é€‚åº”åº¦å‡½æ•°æƒé‡
                sharpe_weight=ACTIVE_CONFIG["sharpe_weight"],
                drawdown_weight=ACTIVE_CONFIG["drawdown_weight"],
                stability_weight=ACTIVE_CONFIG["stability_weight"],
                # GPUä¼˜åŒ–å‚æ•°
                batch_size=ACTIVE_CONFIG["batch_size"],
                early_stop_patience=ACTIVE_CONFIG["early_stop_patience"],
                use_torch_scan=ACTIVE_CONFIG["use_torch_scan"]
            )
            print(f"CUDAé—ä¼ ç®—æ³•é…ç½®: {ga_config}")
            ga = CudaGPUAcceleratedGA(ga_config, gpu_manager)

        # --- 9. æ™ºèƒ½åŠ è½½æˆ–åˆå§‹åŒ–ç§ç¾¤ ---
        if load_checkpoint_path:
            with timer("load_checkpoint", "setup"):
                ga.load_checkpoint(str(load_checkpoint_path))
        else:
            print("åˆå§‹åŒ–æ–°çš„ç§ç¾¤...")
            ga.initialize_population(seed=int(time.time())) # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºç§å­

        # --- 10. å¼€å§‹è¿›åŒ– ---
        with timer("evolution_process", "training"):
            print("å¼€å§‹CUDAåŠ é€Ÿè¿›åŒ–è¿‡ç¨‹...")
            
            # ä½¿ç”¨å›ºå®šçš„æ—¥å¿—æ–‡ä»¶åï¼Œæ‰€æœ‰è®­ç»ƒç»“æœéƒ½è¿½åŠ åˆ°åŒä¸€ä¸ªæ–‡ä»¶
            generation_log_file = output_dir / "training_history_cuda.jsonl"
            print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å°†å†™å…¥: {generation_log_file}")
            
            # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå®éªŒæ€§ï¼‰
            if ACTIVE_CONFIG.get("mixed_precision", False):
                print("ğŸ§ª å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒçš„ä»£ç 
            
            results = ga.evolve(
                train_features,
                train_labels,
                save_checkpoints=ACTIVE_CONFIG["save_checkpoints"],
                checkpoint_dir=checkpoint_dir,
                
                save_generation_results=ACTIVE_CONFIG["save_generation_results"],
                generation_log_file=generation_log_file,
                generation_log_interval=ACTIVE_CONFIG["generation_log_interval"],
                auto_save_best=ACTIVE_CONFIG["auto_save_best"],
                output_dir=output_dir,
            )

        # --- 10. ä¿å­˜æœ€ç»ˆç»“æœ ---
        print("è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆç»“æœ...")
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æœ€ä½³ä¸ªä½“
        best_individual_path = output_dir / f"best_individual_cuda_{timestamp}.npy"
        np.save(best_individual_path, results['best_individual'])
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        config_path = output_dir / f"training_config_cuda_{timestamp}.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(ACTIVE_CONFIG, f, indent=2, ensure_ascii=False)

        # --- 11. è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š ---
        print("="*70)
        print("              CUDA GPUåŠ é€Ÿé—ä¼ ç®—æ³•è®­ç»ƒå®Œæˆ")
        print("="*70)
        print(f"  - ä½¿ç”¨GPU:     {gpu_manager.device}")
        if gpu_manager.device.type == 'cuda':
            print(f"  - GPUåç§°:     {torch.cuda.get_device_name(gpu_manager.device.index)}")
        print(f"  - æœ€ä½³é€‚åº”åº¦:   {results['best_fitness']:.6f}")
        print(f"  - æ€»è®­ç»ƒæ—¶é—´:   {results['total_time']:.2f}ç§’")
        print(f"  - æœ€ç»ˆä»£æ•°:     {results['final_generation']}")
        print(f"  - ç§ç¾¤å¤§å°:     {ACTIVE_CONFIG['population_size']}")
        print(f"  - æœ€ä½³ä¸ªä½“:     {best_individual_path}")
        print(f"  - è®­ç»ƒé…ç½®:     {config_path}")
        print(f"  - å®æ—¶æ—¥å¿—:     {generation_log_file}")
        print(f"  - ç»“æœç›®å½•:     {output_dir}")
        print("="*70)
        
        # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        gpu_alloc, gpu_total, sys_used, sys_total = gpu_manager.get_memory_usage()
        print(f"æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {gpu_alloc:.2f}GB / {gpu_total:.2f}GB")
        print(f"æœ€ç»ˆç³»ç»Ÿå†…å­˜ä½¿ç”¨: {sys_used:.2f}GB / {sys_total:.2f}GB")
        
        # --- 12. æ€§èƒ½åˆ†ææŠ¥å‘Š ---
        if PERFORMANCE_PROFILER_AVAILABLE:
            stop_monitoring()
            print("\n" + "="*80)
            print("ğŸ” æ€§èƒ½åˆ†ææŠ¥å‘Š")
            print("="*80)
            print_summary(detailed=True)
            
            # ä¿å­˜è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
            performance_report_path = output_dir / f"performance_report_cuda_{timestamp}.json"
            save_report(performance_report_path)
            print(f"ğŸ“Š è¯¦ç»†æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {performance_report_path}")
            print("="*80)

    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ¸…ç†GPUç¼“å­˜
        if 'gpu_manager' in locals():
            gpu_manager.clear_cache()
            print("GPUç¼“å­˜å·²æ¸…ç†")


if __name__ == "__main__":
    main()