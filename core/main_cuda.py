"""
CUDAç‰ˆé—ä¼ ç®—æ³•äº¤æ˜“å‘˜è®­ç»ƒä¸»ç¨‹åº
é€‚ç”¨äºNVIDIA RTX 4060ç­‰CUDAå…¼å®¹æ˜¾å¡
"""

import time
from pathlib import Path
import json
import torch
import numpy as np
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# æ£€æŸ¥CUDAå¯ç”¨æ€§
def check_cuda_availability():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ï¼š")
        print("  1. æ˜¯å¦å®‰è£…äº†NVIDIAé©±åŠ¨")
        print("  2. æ˜¯å¦å®‰è£…äº†CUDA Toolkit")
        print("  3. æ˜¯å¦å®‰è£…äº†æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬")
        print("  å®‰è£…å‘½ä»¤: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return False
    
    device_count = torch.cuda.device_count()
    current_device = torch.cuda.current_device()
    device_name = torch.cuda.get_device_name(current_device)
    
    print("âœ… CUDAç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    print(f"  è®¾å¤‡æ•°é‡: {device_count}")
    print(f"  å½“å‰è®¾å¤‡: {current_device}")
    print(f"  è®¾å¤‡åç§°: {device_name}")
    print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"  æ˜¾å­˜å®¹é‡: {torch.cuda.get_device_properties(current_device).total_memory / 1024**3:.1f} GB")
    
    return True

# ç®€åŒ–çš„GPUç®¡ç†å™¨ï¼ˆé€‚ç”¨äºCUDAï¼‰
class CudaGPUManager:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def to_gpu(self, tensor):
        """å°†å¼ é‡ç§»åŠ¨åˆ°GPU"""
        if isinstance(tensor, torch.Tensor):
            return tensor.to(self.device)
        elif isinstance(tensor, np.ndarray):
            return torch.from_numpy(tensor).to(self.device)
        else:
            return torch.tensor(tensor).to(self.device)
    
    def to_cpu(self, tensor):
        """å°†å¼ é‡ç§»åŠ¨åˆ°CPU"""
        if isinstance(tensor, torch.Tensor):
            return tensor.cpu().numpy()
        return tensor
    
    def clear_cache(self):
        """æ¸…ç†GPUç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def get_memory_usage(self):
        """è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3      # GB
            return allocated, cached
        return 0.0, 0.0

# ç®€åŒ–çš„æ•°æ®å¤„ç†å™¨ï¼ˆé€‚ç”¨äºCUDAï¼‰
class CudaDataProcessor:
    def __init__(self, window_size=350, normalization_method='rolling', gpu_manager=None):
        self.window_size = window_size
        self.normalization_method = normalization_method
        self.gpu_manager = gpu_manager or CudaGPUManager()
    
    def load_and_process_data(self, data_file):
        """åŠ è½½å’Œå¤„ç†æ•°æ®"""
        print(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
        
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ•°æ®åŠ è½½é€»è¾‘
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        n_samples = 1000
        feature_dim = 1400
        
        # åˆ›å»ºæ¨¡æ‹Ÿç‰¹å¾æ•°æ®
        features = np.random.randn(n_samples, feature_dim).astype(np.float32)
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ ‡ç­¾æ•°æ®ï¼ˆä»·æ ¼ï¼‰
        labels = np.cumsum(np.random.randn(n_samples) * 0.01) + 100
        labels = labels.astype(np.float32)
        
        # è½¬æ¢ä¸ºGPUå¼ é‡
        features_tensor = self.gpu_manager.to_gpu(features)
        labels_tensor = self.gpu_manager.to_gpu(labels)
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ: ç‰¹å¾ {features_tensor.shape}, æ ‡ç­¾ {labels_tensor.shape}")
        return features_tensor, labels_tensor

# ç®€åŒ–çš„é—ä¼ ç®—æ³•é…ç½®
class CudaGAConfig:
    def __init__(self, population_size=500, max_generations=100, mutation_rate=0.01, 
                 crossover_rate=0.8, elite_ratio=0.1, feature_dim=1400):
        self.population_size = population_size
        self.max_generations = max_generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elite_ratio = elite_ratio
        self.feature_dim = feature_dim
        self.gene_length = feature_dim + 5  # ç‰¹å¾æƒé‡ + 5ä¸ªé£é™©å‚æ•°

# ç®€åŒ–çš„CUDAé—ä¼ ç®—æ³•
class CudaGeneticAlgorithm:
    def __init__(self, config, gpu_manager):
        self.config = config
        self.gpu_manager = gpu_manager
        self.device = gpu_manager.device
        self.population = None
        self.best_individual = None
        self.best_fitness = float('-inf')
        self.generation = 0
    
    def initialize_population(self, seed=None):
        """åˆå§‹åŒ–ç§ç¾¤"""
        if seed is not None:
            torch.manual_seed(seed)
            np.random.seed(seed)
        
        self.population = torch.randn(
            self.config.population_size, 
            self.config.gene_length,
            device=self.device,
            dtype=torch.float32
        ) * 0.1
        
        print(f"ç§ç¾¤åˆå§‹åŒ–å®Œæˆ: {self.population.shape}")
        return self.population
    
    def evaluate_fitness(self, features, prices):
        """ç®€åŒ–çš„é€‚åº”åº¦è¯„ä¼°"""
        # æå–æƒé‡
        weights = self.population[:, :self.config.feature_dim]
        
        # è®¡ç®—å†³ç­–åˆ†æ•°
        scores = torch.mm(weights, features.T)
        
        # ç®€åŒ–çš„é€‚åº”åº¦è®¡ç®—ï¼ˆè¿™é‡Œåº”è¯¥å®ç°å®é™…çš„äº¤æ˜“ç­–ç•¥è¯„ä¼°ï¼‰
        fitness = torch.mean(scores, dim=1) + torch.randn(self.config.population_size, device=self.device) * 0.01
        
        return fitness
    
    def evolve_one_generation(self, features, prices):
        """è¿›åŒ–ä¸€ä»£"""
        start_time = time.time()
        
        # é€‚åº”åº¦è¯„ä¼°
        fitness = self.evaluate_fitness(features, prices)
        
        # æ›´æ–°æœ€ä½³ä¸ªä½“
        best_idx = torch.argmax(fitness)
        current_best_fitness = fitness[best_idx].item()
        
        if current_best_fitness > self.best_fitness:
            self.best_fitness = current_best_fitness
            self.best_individual = self.population[best_idx].clone()
        
        # ç®€åŒ–çš„é€‰æ‹©ã€äº¤å‰ã€å˜å¼‚
        # ç²¾è‹±ä¿ç•™
        elite_count = max(1, int(self.config.population_size * self.config.elite_ratio))
        elite_indices = torch.topk(fitness, elite_count).indices
        
        # åˆ›å»ºæ–°ç§ç¾¤
        new_population = torch.zeros_like(self.population)
        new_population[:elite_count] = self.population[elite_indices]
        
        # éšæœºäº¤å‰å’Œå˜å¼‚
        for i in range(elite_count, self.config.population_size):
            # éšæœºé€‰æ‹©ä¸¤ä¸ªçˆ¶ä»£
            parent1_idx = torch.randint(0, self.config.population_size, (1,)).item()
            parent2_idx = torch.randint(0, self.config.population_size, (1,)).item()
            
            # äº¤å‰
            if torch.rand(1) < self.config.crossover_rate:
                mask = torch.rand(self.config.gene_length, device=self.device) < 0.5
                child = torch.where(mask, self.population[parent1_idx], self.population[parent2_idx])
            else:
                child = self.population[parent1_idx].clone()
            
            # å˜å¼‚
            mutation_mask = torch.rand(self.config.gene_length, device=self.device) < self.config.mutation_rate
            mutation = torch.randn(self.config.gene_length, device=self.device) * 0.01
            child[mutation_mask] += mutation[mutation_mask]
            
            new_population[i] = child
        
        self.population = new_population
        self.generation += 1
        
        # ç»Ÿè®¡ä¿¡æ¯
        gen_time = time.time() - start_time
        allocated_memory, cached_memory = self.gpu_manager.get_memory_usage()
        
        stats = {
            'generation': self.generation,
            'best_fitness': current_best_fitness,
            'mean_fitness': torch.mean(fitness).item(),
            'std_fitness': torch.std(fitness).item(),
            'generation_time': gen_time,
            'system_memory_gb': allocated_memory,
            'mean_sharpe_ratio': current_best_fitness * 0.8,  # æ¨¡æ‹Ÿæ•°æ®
            'mean_sortino_ratio': current_best_fitness * 1.2   # æ¨¡æ‹Ÿæ•°æ®
        }
        
        return stats
    
    def evolve(self, features, prices, save_checkpoints=False, checkpoint_dir=None,
               checkpoint_interval=50, continuous_training=False,
               save_generation_results=False, generation_log_file=None,
               generation_log_interval=1, auto_save_best=False,
               output_dir=None):
        """æ‰§è¡Œè¿›åŒ–è¿‡ç¨‹"""
        
        def save_generation_log(stats_data):
            """ä¿å­˜æ¯ä»£ç»“æœåˆ°æ—¥å¿—æ–‡ä»¶"""
            if save_generation_results and generation_log_file:
                try:
                    stats_data['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
                    with open(generation_log_file, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(stats_data, ensure_ascii=False) + '\n')
                except Exception as e:
                    print(f"è­¦å‘Šï¼šä¿å­˜æ¯ä»£ç»“æœå¤±è´¥: {e}")
        
        total_start_time = time.time()
        fitness_history = []
        
        print("å¼€å§‹CUDAåŠ é€Ÿè¿›åŒ–...")
        
        for gen in range(self.config.max_generations):
            stats = self.evolve_one_generation(features, prices)
            fitness_history.append(stats)
            
            print(f"ç¬¬ {stats['generation']} ä»£: "
                  f"æœ€ä½³é€‚åº”åº¦={stats['best_fitness']:.4f}, "
                  f"å¹³å‡é€‚åº”åº¦={stats['mean_fitness']:.4f}, "
                  f"ç”¨æ—¶={stats['generation_time']:.2f}ç§’, "
                  f"æ˜¾å­˜={stats['system_memory_gb']:.2f}GB")
            
            # ä¿å­˜æ¯ä»£ç»“æœ
            if save_generation_results and stats['generation'] % generation_log_interval == 0:
                save_generation_log(stats)
            
            # æ¸…ç†æ˜¾å­˜
            if gen % 10 == 0:
                self.gpu_manager.clear_cache()
        
        total_time = time.time() - total_start_time
        
        return {
            'best_individual': self.gpu_manager.to_cpu(self.best_individual),
            'best_fitness': self.best_fitness,
            'fitness_history': fitness_history,
            'total_time': total_time,
            'final_generation': self.generation
        }

def main():
    """ä¸»å‡½æ•° - RTX 4060ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # ==============================================================================
    # ======================= RTX 4060 è®­ç»ƒå‚æ•°é…ç½® ============================
    # ==============================================================================
    TRAINING_CONFIG = {
        # --- æ•°æ®å‚æ•° ---
        "data_directory": "../data",
        "window_size": 350,
        "normalization": "rolling",

        # --- é—ä¼ ç®—æ³•å‚æ•°ï¼ˆRTX 4060ä¼˜åŒ–ï¼‰---
        "population_size": 1000,     # RTX 4060å¯ä»¥æ”¯æŒæ›´å¤§çš„ç§ç¾¤
        "generations": 100,          # è®­ç»ƒä»£æ•°
        "mutation_rate": 0.01,
        "crossover_rate": 0.8,
        "elite_ratio": 0.1,

        # --- è¾“å‡ºå‚æ•° ---
        "save_checkpoints": True,
        "checkpoint_interval": 20,
        "results_dir": "../results",
        "save_generation_results": True,
        "generation_log_interval": 1,
        "auto_save_best": True,
    }
    # ==============================================================================
    
    print("=== RTX 4060 CUDAåŠ é€Ÿé—ä¼ ç®—æ³•äº¤æ˜“å‘˜è®­ç»ƒ ===")
    
    # æ£€æŸ¥CUDAç¯å¢ƒ
    if not check_cuda_availability():
        print("âŒ CUDAç¯å¢ƒä¸å¯ç”¨ï¼Œç¨‹åºé€€å‡º")
        return
    
    # è®¾ç½®è·¯å¾„
    output_dir = Path(TRAINING_CONFIG["results_dir"])
    checkpoint_dir = output_dir / "checkpoints"
    data_dir = Path(TRAINING_CONFIG["data_directory"])
    
    output_dir.mkdir(exist_ok=True)
    checkpoint_dir.mkdir(exist_ok=True)
    
    print("\n--- è®­ç»ƒå‚æ•° ---")
    for key, value in TRAINING_CONFIG.items():
        print(f"{key}: {value}")
    print("--------------------\n")
    
    try:
        # åˆå§‹åŒ–GPUç®¡ç†å™¨
        gpu_manager = CudaGPUManager()
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {gpu_manager.device}")
        
        # æ•°æ®å¤„ç†
        processor = CudaDataProcessor(
            window_size=TRAINING_CONFIG["window_size"],
            normalization_method=TRAINING_CONFIG["normalization"],
            gpu_manager=gpu_manager
        )
        
        # æ¨¡æ‹Ÿæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥ä»data_dirä¸­é€‰æ‹©ï¼‰
        data_file = data_dir / "sample_data.csv"
        train_features, train_labels = processor.load_and_process_data(data_file)
        
        # é…ç½®é—ä¼ ç®—æ³•
        ga_config = CudaGAConfig(
            population_size=TRAINING_CONFIG["population_size"],
            max_generations=TRAINING_CONFIG["generations"],
            mutation_rate=TRAINING_CONFIG["mutation_rate"],
            crossover_rate=TRAINING_CONFIG["crossover_rate"],
            elite_ratio=TRAINING_CONFIG["elite_ratio"],
            feature_dim=train_features.shape[1]
        )
        
        # åˆå§‹åŒ–é—ä¼ ç®—æ³•
        ga = CudaGeneticAlgorithm(ga_config, gpu_manager)
        ga.initialize_population(seed=int(time.time()))
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶
        generation_log_file = output_dir / "training_history.jsonl"
        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å°†å†™å…¥: {generation_log_file}")
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹CUDAåŠ é€Ÿè®­ç»ƒ...")
        results = ga.evolve(
            train_features,
            train_labels,
            save_checkpoints=TRAINING_CONFIG["save_checkpoints"],
            checkpoint_dir=checkpoint_dir,
            checkpoint_interval=TRAINING_CONFIG["checkpoint_interval"],
            save_generation_results=TRAINING_CONFIG["save_generation_results"],
            generation_log_file=generation_log_file,
            generation_log_interval=TRAINING_CONFIG["generation_log_interval"],
            auto_save_best=TRAINING_CONFIG["auto_save_best"],
            output_dir=output_dir
        )
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        best_individual_path = output_dir / f"best_individual_{timestamp}.npy"
        np.save(best_individual_path, results['best_individual'])
        
        # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        print("="*60)
        print("           RTX 4060 CUDAè®­ç»ƒå®Œæˆ")
        print("="*60)
        print(f"  - æœ€ä½³é€‚åº”åº¦: {results['best_fitness']:.4f}")
        print(f"  - æ€»è®­ç»ƒæ—¶é—´: {results['total_time']:.2f}ç§’")
        print(f"  - æœ€ç»ˆä»£æ•°:   {results['final_generation']}")
        print(f"  - æœ€ä½³ä¸ªä½“:   {best_individual_path}")
        print(f"  - è®­ç»ƒæ—¥å¿—:   {generation_log_file}")
        print("="*60)
        
        # æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        allocated, cached = gpu_manager.get_memory_usage()
        print(f"  - æ˜¾å­˜ä½¿ç”¨:   {allocated:.2f}GB / {cached:.2f}GB (å·²åˆ†é…/å·²ç¼“å­˜)")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        # æ¸…ç†æ˜¾å­˜
        if 'gpu_manager' in locals():
            gpu_manager.clear_cache()

if __name__ == "__main__":
    main()