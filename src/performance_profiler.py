#!/usr/bin/env python3
"""
æ€§èƒ½åˆ†æå™¨
ç”¨äºç›‘æ§é—ä¼ ç®—æ³•å„éƒ¨åˆ†çš„è¿è¡Œæ—¶é—´å’Œæ€§èƒ½ç“¶é¢ˆ
"""

import time
import torch
import psutil
import threading
from collections import defaultdict, deque
from typing import Dict, List, Optional, Any
from contextlib import contextmanager
from dataclasses import dataclass
import json
from pathlib import Path


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    name: str
    total_time: float = 0.0
    call_count: int = 0
    avg_time: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    last_time: float = 0.0
    
    def update(self, elapsed_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.total_time += elapsed_time
        self.call_count += 1
        self.avg_time = self.total_time / self.call_count
        self.min_time = min(self.min_time, elapsed_time)
        self.max_time = max(self.max_time, elapsed_time)
        self.last_time = elapsed_time
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'name': self.name,
            'total_time': self.total_time,
            'call_count': self.call_count,
            'avg_time': self.avg_time,
            'min_time': self.min_time if self.min_time != float('inf') else 0.0,
            'max_time': self.max_time,
            'last_time': self.last_time
        }


class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, enable_gpu_monitoring: bool = True):
        """
        åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨
        
        Args:
            enable_gpu_monitoring: æ˜¯å¦å¯ç”¨GPUç›‘æ§
        """
        self.metrics: Dict[str, PerformanceMetrics] = {}
        self.enable_gpu_monitoring = enable_gpu_monitoring and torch.cuda.is_available()
        self.start_time = time.time()
        
        # GPUå†…å­˜ç›‘æ§
        self.gpu_memory_history = deque(maxlen=1000)
        self.system_memory_history = deque(maxlen=1000)
        
        # ç›‘æ§çº¿ç¨‹
        self._monitoring = False
        self._monitor_thread = None
        
        # åµŒå¥—è®¡æ—¶å™¨æ ˆ
        self._timer_stack = []
        
        print(f"ğŸ” æ€§èƒ½åˆ†æå™¨å·²å¯åŠ¨")
        print(f"   GPUç›‘æ§: {'å¯ç”¨' if self.enable_gpu_monitoring else 'ç¦ç”¨'}")
    
    @contextmanager
    def timer(self, name: str, category: str = "general"):
        """
        è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            name: è®¡æ—¶å™¨åç§°
            category: åˆ†ç±»
        """
        full_name = f"{category}.{name}" if category != "general" else name
        
        # GPUåŒæ­¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.enable_gpu_monitoring:
            torch.cuda.synchronize()
        
        start_time = time.time()
        self._timer_stack.append((full_name, start_time))
        
        try:
            yield
        finally:
            # GPUåŒæ­¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.enable_gpu_monitoring:
                torch.cuda.synchronize()
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            # ç§»é™¤æ ˆé¡¶
            self._timer_stack.pop()
            
            # æ›´æ–°æŒ‡æ ‡
            if full_name not in self.metrics:
                self.metrics[full_name] = PerformanceMetrics(full_name)
            
            self.metrics[full_name].update(elapsed_time)
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            self._record_memory_usage()
    
    def start_monitoring(self, interval: float = 1.0):
        """
        å¼€å§‹åå°ç›‘æ§
        
        Args:
            interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
        """
        if self._monitoring:
            return
        
        self._monitoring = True
        self._monitor_thread = threading.Thread(
            target=self._monitor_loop, 
            args=(interval,),
            daemon=True
        )
        self._monitor_thread.start()
        print(f"ğŸ“Š åå°ç›‘æ§å·²å¯åŠ¨ (é—´éš”: {interval}s)")
    
    def stop_monitoring(self):
        """åœæ­¢åå°ç›‘æ§"""
        self._monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=2.0)
        print("ğŸ“Š åå°ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self, interval: float):
        """ç›‘æ§å¾ªç¯"""
        while self._monitoring:
            self._record_memory_usage()
            time.sleep(interval)
    
    def _record_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        timestamp = time.time() - self.start_time
        
        # ç³»ç»Ÿå†…å­˜
        system_memory = psutil.virtual_memory()
        self.system_memory_history.append({
            'timestamp': timestamp,
            'used_gb': system_memory.used / 1e9,
            'percent': system_memory.percent
        })
        
        # GPUå†…å­˜
        if self.enable_gpu_monitoring:
            gpu_allocated = torch.cuda.memory_allocated() / 1e9
            gpu_reserved = torch.cuda.memory_reserved() / 1e9
            self.gpu_memory_history.append({
                'timestamp': timestamp,
                'allocated_gb': gpu_allocated,
                'reserved_gb': gpu_reserved
            })
    
    def get_summary(self, sort_by: str = "total_time") -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½æ€»ç»“
        
        Args:
            sort_by: æ’åºæ–¹å¼ ('total_time', 'avg_time', 'call_count')
        
        Returns:
            æ€§èƒ½æ€»ç»“å­—å…¸
        """
        # æŒ‰ç±»åˆ«åˆ†ç»„
        categories = defaultdict(list)
        for name, metrics in self.metrics.items():
            if '.' in name:
                category, method = name.split('.', 1)
            else:
                category, method = 'general', name
            categories[category].append(metrics)
        
        # æ’åº
        for category in categories:
            categories[category].sort(key=lambda x: getattr(x, sort_by), reverse=True)
        
        # è®¡ç®—æ€»æ—¶é—´
        total_runtime = time.time() - self.start_time
        
        # å†…å­˜ç»Ÿè®¡
        memory_stats = self._get_memory_stats()
        
        return {
            'total_runtime': total_runtime,
            'categories': {cat: [m.to_dict() for m in metrics] for cat, metrics in categories.items()},
            'memory_stats': memory_stats,
            'top_bottlenecks': self._get_top_bottlenecks(5)
        }
    
    def _get_memory_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç»Ÿè®¡"""
        stats = {
            'system_memory': {
                'current_gb': psutil.virtual_memory().used / 1e9,
                'peak_gb': max([m['used_gb'] for m in self.system_memory_history], default=0),
                'history_length': len(self.system_memory_history)
            }
        }
        
        if self.enable_gpu_monitoring and self.gpu_memory_history:
            stats['gpu_memory'] = {
                'current_allocated_gb': torch.cuda.memory_allocated() / 1e9,
                'current_reserved_gb': torch.cuda.memory_reserved() / 1e9,
                'peak_allocated_gb': max([m['allocated_gb'] for m in self.gpu_memory_history], default=0),
                'peak_reserved_gb': max([m['reserved_gb'] for m in self.gpu_memory_history], default=0),
                'history_length': len(self.gpu_memory_history)
            }
        
        return stats
    
    def _get_top_bottlenecks(self, top_n: int = 5) -> List[Dict[str, Any]]:
        """è·å–æ€§èƒ½ç“¶é¢ˆ"""
        sorted_metrics = sorted(
            self.metrics.values(), 
            key=lambda x: x.total_time, 
            reverse=True
        )
        
        return [m.to_dict() for m in sorted_metrics[:top_n]]
    
    def print_summary(self, detailed: bool = True):
        """
        æ‰“å°æ€§èƒ½æ€»ç»“
        
        Args:
            detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        summary = self.get_summary()
        total_time = summary['total_runtime']
        
        print("\n" + "="*80)
        print("ğŸ” æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("="*80)
        print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        # å†…å­˜ç»Ÿè®¡
        memory_stats = summary['memory_stats']
        print(f"\nğŸ“Š å†…å­˜ä½¿ç”¨:")
        print(f"   ç³»ç»Ÿå†…å­˜: {memory_stats['system_memory']['current_gb']:.2f}GB (å³°å€¼: {memory_stats['system_memory']['peak_gb']:.2f}GB)")
        
        if 'gpu_memory' in memory_stats:
            gpu_mem = memory_stats['gpu_memory']
            print(f"   GPUå†…å­˜:  {gpu_mem['current_allocated_gb']:.2f}GB / {gpu_mem['current_reserved_gb']:.2f}GB (å³°å€¼: {gpu_mem['peak_allocated_gb']:.2f}GB / {gpu_mem['peak_reserved_gb']:.2f}GB)")
        
        # æ€§èƒ½ç“¶é¢ˆ
        print(f"\nğŸš¨ æ€§èƒ½ç“¶é¢ˆ (Top 5):")
        print(f"{'åç§°':<40} {'æ€»æ—¶é—´':<10} {'è°ƒç”¨æ¬¡æ•°':<8} {'å¹³å‡æ—¶é—´':<10} {'å æ¯”':<8}")
        print("-" * 80)
        
        for bottleneck in summary['top_bottlenecks']:
            percentage = (bottleneck['total_time'] / total_time) * 100
            print(f"{bottleneck['name']:<40} {bottleneck['total_time']:<10.3f} {bottleneck['call_count']:<8} {bottleneck['avg_time']:<10.3f} {percentage:<8.1f}%")
        
        if detailed:
            # æŒ‰ç±»åˆ«è¯¦ç»†æ˜¾ç¤º
            print(f"\nğŸ“‹ è¯¦ç»†åˆ†æ:")
            for category, metrics in summary['categories'].items():
                print(f"\nğŸ”¸ {category.upper()}:")
                print(f"{'æ–¹æ³•':<30} {'æ€»æ—¶é—´':<10} {'è°ƒç”¨æ¬¡æ•°':<8} {'å¹³å‡æ—¶é—´':<10} {'æœ€å°':<8} {'æœ€å¤§':<8}")
                print("-" * 80)
                
                for metric in metrics:
                    print(f"{metric['name'].split('.')[-1]:<30} {metric['total_time']:<10.3f} {metric['call_count']:<8} {metric['avg_time']:<10.3f} {metric['min_time']:<8.3f} {metric['max_time']:<8.3f}")
        
        print("="*80)
    
    def save_report(self, filepath: Path):
        """
        ä¿å­˜æ€§èƒ½æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        summary = self.get_summary()
        
        # æ·»åŠ æ—¶é—´æˆ³
        summary['timestamp'] = time.strftime("%Y-%m-%d %H:%M:%S")
        summary['memory_history'] = {
            'system': list(self.system_memory_history),
            'gpu': list(self.gpu_memory_history) if self.enable_gpu_monitoring else []
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡æ•°æ®"""
        self.metrics.clear()
        self.gpu_memory_history.clear()
        self.system_memory_history.clear()
        self.start_time = time.time()
        print("ğŸ”„ æ€§èƒ½åˆ†æå™¨å·²é‡ç½®")


# å…¨å±€æ€§èƒ½åˆ†æå™¨å®ä¾‹
_global_profiler: Optional[PerformanceProfiler] = None


def get_profiler() -> PerformanceProfiler:
    """è·å–å…¨å±€æ€§èƒ½åˆ†æå™¨"""
    global _global_profiler
    if _global_profiler is None:
        _global_profiler = PerformanceProfiler()
    return _global_profiler


def profile(name: str, category: str = "general"):
    """
    æ€§èƒ½åˆ†æè£…é¥°å™¨
    
    Args:
        name: æ–¹æ³•åç§°
        category: åˆ†ç±»
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            with get_profiler().timer(name, category):
                return func(*args, **kwargs)
        return wrapper
    return decorator


# ä¾¿æ·å‡½æ•°
def timer(name: str, category: str = "general"):
    """è·å–è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    return get_profiler().timer(name, category)


def start_monitoring(interval: float = 1.0):
    """å¼€å§‹æ€§èƒ½ç›‘æ§"""
    get_profiler().start_monitoring(interval)


def stop_monitoring():
    """åœæ­¢æ€§èƒ½ç›‘æ§"""
    get_profiler().stop_monitoring()


def print_summary(detailed: bool = True):
    """æ‰“å°æ€§èƒ½æ€»ç»“"""
    get_profiler().print_summary(detailed)


def save_report(filepath: Path):
    """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
    get_profiler().save_report(filepath)


def reset_profiler():
    """é‡ç½®æ€§èƒ½åˆ†æå™¨"""
    get_profiler().reset()


if __name__ == "__main__":
    # æµ‹è¯•æ€§èƒ½åˆ†æå™¨
    print("=== æ€§èƒ½åˆ†æå™¨æµ‹è¯• ===")
    
    profiler = PerformanceProfiler()
    profiler.start_monitoring(0.5)
    
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    with profiler.timer("test_operation", "test"):
        time.sleep(0.1)
        
        with profiler.timer("sub_operation", "test"):
            time.sleep(0.05)
    
    with profiler.timer("another_operation", "test"):
        time.sleep(0.2)
    
    # æ¨¡æ‹ŸGPUæ“ä½œï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        with profiler.timer("gpu_operation", "gpu"):
            x = torch.randn(1000, 1000, device='cuda')
            y = torch.matmul(x, x.T)
            torch.cuda.synchronize()
    
    time.sleep(1)  # è®©ç›‘æ§æ”¶é›†ä¸€äº›æ•°æ®
    
    profiler.stop_monitoring()
    profiler.print_summary()
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path("performance_test_report.json")
    profiler.save_report(report_path)
    
    print("æ€§èƒ½åˆ†æå™¨æµ‹è¯•å®Œæˆï¼")