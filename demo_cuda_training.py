#!/usr/bin/env python3
"""
CUDAç‰ˆé—ä¼ ç®—æ³•äº¤æ˜“å‘˜è®­ç»ƒæ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•åœ¨CUDA 12.9ç¯å¢ƒä¸‹è¿è¡Œè®­ç»ƒ
"""

import sys
import time
import numpy as np
import pandas as pd
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / 'src'))

def create_demo_data():
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„äº¤æ˜“æ•°æ®"""
    print("åˆ›å»ºæ¼”ç¤ºäº¤æ˜“æ•°æ®...")
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    np.random.seed(42)
    
    # ç”Ÿæˆ5000ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
    n_days = 5000
    base_price = 2000.0
    
    # æ¨¡æ‹Ÿä»·æ ¼éšæœºæ¸¸èµ°
    daily_returns = np.random.normal(0.0005, 0.02, n_days)  # å¹³å‡æ—¥æ”¶ç›Š0.05%ï¼Œæ³¢åŠ¨ç‡2%
    
    # æ·»åŠ ä¸€äº›è¶‹åŠ¿å’Œå‘¨æœŸæ€§
    trend = np.linspace(0, 0.5, n_days)  # é•¿æœŸä¸Šå‡è¶‹åŠ¿
    cycle = 0.1 * np.sin(2 * np.pi * np.arange(n_days) / 252)  # å¹´åº¦å‘¨æœŸ
    
    # è®¡ç®—ç´¯ç§¯ä»·æ ¼
    price_changes = daily_returns + trend/n_days + cycle/n_days
    close_prices = base_price * np.exp(np.cumsum(price_changes))
    
    # ç”ŸæˆOHLCæ•°æ®
    opens = np.roll(close_prices, 1)
    opens[0] = base_price
    
    # ç”Ÿæˆé«˜ä½ä»·ï¼ˆåŸºäºå¼€ç›˜å’Œæ”¶ç›˜ä»·ï¼‰
    highs = np.maximum(opens, close_prices) * (1 + np.random.exponential(0.005, n_days))
    lows = np.minimum(opens, close_prices) * (1 - np.random.exponential(0.005, n_days))
    
    # åˆ›å»ºDataFrame
    data = pd.DataFrame({
        'open': opens,
        'high': highs,
        'low': lows,
        'close': close_prices
    })
    
    # ä¿å­˜æ•°æ®
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    
    data_file = data_dir / 'demo_trading_data.csv'
    data.to_csv(data_file, index=False)
    
    print(f"æ¼”ç¤ºæ•°æ®å·²ä¿å­˜: {data_file}")
    print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"ä»·æ ¼èŒƒå›´: {close_prices.min():.2f} - {close_prices.max():.2f}")
    
    return data_file


def demo_cuda_environment():
    """æ¼”ç¤ºCUDAç¯å¢ƒæ£€æŸ¥"""
    print("\n=== CUDAç¯å¢ƒæ£€æŸ¥ ===")
    
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"GPU {i}: {props.name} ({props.total_memory / 1e9:.1f}GB)")
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    return True


def demo_cuda_modules():
    """æ¼”ç¤ºCUDAæ¨¡å—åŠŸèƒ½"""
    print("\n=== CUDAæ¨¡å—æµ‹è¯• ===")
    
    try:
        from cuda_gpu_utils import get_cuda_gpu_manager, check_cuda_compatibility
        from cuda_accelerated_ga import CudaGAConfig, CudaGPUAcceleratedGA
        from data_processor import GPUDataProcessor
        
        print("âœ… æ‰€æœ‰CUDAæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥CUDAå…¼å®¹æ€§
        cuda_info = check_cuda_compatibility()
        print(f"CUDAå…¼å®¹æ€§æ£€æŸ¥å®Œæˆ: {cuda_info['cuda_available']}")
        
        # åˆå§‹åŒ–GPUç®¡ç†å™¨
        gpu_manager = get_cuda_gpu_manager()
        print(f"GPUç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ: {gpu_manager.device}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        return False


def demo_quick_training():
    """æ¼”ç¤ºå¿«é€Ÿè®­ç»ƒ"""
    print("\n=== å¿«é€Ÿè®­ç»ƒæ¼”ç¤º ===")
    
    try:
        from cuda_gpu_utils import get_cuda_gpu_manager
        from cuda_accelerated_ga import CudaGAConfig, CudaGPUAcceleratedGA
        from data_processor import GPUDataProcessor
        
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®
        data_file = create_demo_data()
        
        # åˆå§‹åŒ–GPUç®¡ç†å™¨
        gpu_manager = get_cuda_gpu_manager()
        print(f"ä½¿ç”¨è®¾å¤‡: {gpu_manager.device}")
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        processor = GPUDataProcessor(
            gpu_manager=gpu_manager,
            window_size=100,  # è¾ƒå°çš„çª—å£ç”¨äºæ¼”ç¤º
            normalization_method='relative'
        )
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        print("åŠ è½½å’Œå¤„ç†æ•°æ®...")
        features, labels = processor.load_and_process_data(str(data_file))
        print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}, æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        
        # åˆ›å»ºé—ä¼ ç®—æ³•é…ç½®ï¼ˆå°è§„æ¨¡ç”¨äºæ¼”ç¤ºï¼‰
        config = CudaGAConfig(
            population_size=50,      # å°ç§ç¾¤
            max_generations=10,      # å°‘ä»£æ•°
            feature_dim=features.shape[1],
            mutation_rate=0.02,
            crossover_rate=0.8,
            elite_ratio=0.2,
            batch_size=500,
            use_torch_scan=True
        )
        
        print(f"é—ä¼ ç®—æ³•é…ç½®: ç§ç¾¤{config.population_size}, ä»£æ•°{config.max_generations}")
        
        # åˆå§‹åŒ–é—ä¼ ç®—æ³•
        ga = CudaGPUAcceleratedGA(config, gpu_manager)
        ga.initialize_population(seed=42)
        
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹å¿«é€Ÿè®­ç»ƒæ¼”ç¤º...")
        start_time = time.time()
        
        results = ga.evolve(features, labels)
        
        training_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print("\n=== è®­ç»ƒç»“æœ ===")
        print(f"æœ€ä½³é€‚åº”åº¦: {results['best_fitness']:.6f}")
        print(f"è®­ç»ƒä»£æ•°: {results['final_generation']}")
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"å¹³å‡æ¯ä»£æ—¶é—´: {training_time/results['final_generation']:.3f}ç§’")
        
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨
        if gpu_manager.device.type == 'cuda':
            gpu_alloc, gpu_total, sys_used, sys_total = gpu_manager.get_memory_usage()
            print(f"GPUå†…å­˜ä½¿ç”¨: {gpu_alloc:.2f}GB / {gpu_total:.2f}GB")
        
        print("âœ… å¿«é€Ÿè®­ç»ƒæ¼”ç¤ºå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def demo_performance_comparison():
    """æ¼”ç¤ºCPU vs GPUæ€§èƒ½å¯¹æ¯”"""
    print("\n=== æ€§èƒ½å¯¹æ¯”æ¼”ç¤º ===")
    
    try:
        import torch
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        size = 2000
        x = torch.randn(size, size)
        y = torch.randn(size, size)
        
        # CPUæµ‹è¯•
        print("CPUçŸ©é˜µä¹˜æ³•æµ‹è¯•...")
        start_time = time.time()
        z_cpu = torch.matmul(x, y)
        cpu_time = time.time() - start_time
        print(f"CPUæ—¶é—´: {cpu_time:.4f}ç§’")
        
        # GPUæµ‹è¯•
        if torch.cuda.is_available():
            print("GPUçŸ©é˜µä¹˜æ³•æµ‹è¯•...")
            x_gpu = x.cuda()
            y_gpu = y.cuda()
            
            # é¢„çƒ­
            _ = torch.matmul(x_gpu, y_gpu)
            torch.cuda.synchronize()
            
            start_time = time.time()
            z_gpu = torch.matmul(x_gpu, y_gpu)
            torch.cuda.synchronize()
            gpu_time = time.time() - start_time
            
            print(f"GPUæ—¶é—´: {gpu_time:.4f}ç§’")
            print(f"åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§
            if torch.allclose(z_cpu, z_gpu.cpu(), rtol=1e-4):
                print("âœ… è®¡ç®—ç»“æœä¸€è‡´")
            else:
                print("âš ï¸  è®¡ç®—ç»“æœå­˜åœ¨å·®å¼‚")
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½å¯¹æ¯”å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ CUDAç‰ˆé—ä¼ ç®—æ³•äº¤æ˜“å‘˜è®­ç»ƒæ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not demo_cuda_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–")
        return
    
    # æµ‹è¯•æ¨¡å—
    if not demo_cuda_modules():
        print("âŒ æ¨¡å—æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        return
    
    # æ€§èƒ½å¯¹æ¯”
    demo_performance_comparison()
    
    # è¯¢é—®æ˜¯å¦è¿è¡Œè®­ç»ƒæ¼”ç¤º
    print("\n" + "=" * 50)
    print("æ˜¯å¦è¿è¡Œå¿«é€Ÿè®­ç»ƒæ¼”ç¤ºï¼Ÿ")
    print("è¿™å°†åˆ›å»ºæ¼”ç¤ºæ•°æ®å¹¶è¿è¡Œä¸€ä¸ªå°è§„æ¨¡çš„è®­ç»ƒè¿‡ç¨‹")
    response = input("è¾“å…¥ 'y' ç»§ç»­ï¼Œå…¶ä»–é”®è·³è¿‡: ").strip().lower()
    
    if response == 'y':
        if demo_quick_training():
            print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
            print("\næ¥ä¸‹æ¥æ‚¨å¯ä»¥ï¼š")
            print("1. è¿è¡Œå®Œæ•´è®­ç»ƒ: python core/main_cuda.py")
            print("2. ä¿®æ”¹é…ç½®å‚æ•°ä»¥é€‚åº”æ‚¨çš„ç¡¬ä»¶")
            print("3. ä½¿ç”¨æ‚¨è‡ªå·±çš„äº¤æ˜“æ•°æ®")
        else:
            print("\nâŒ è®­ç»ƒæ¼”ç¤ºå¤±è´¥")
    else:
        print("\næ¼”ç¤ºç»“æŸ")
    
    print("\næœ‰ç”¨çš„å‘½ä»¤ï¼š")
    print("  nvidia-smi                    # æŸ¥çœ‹GPUçŠ¶æ€")
    print("  python test_cuda_environment.py  # å®Œæ•´ç¯å¢ƒæµ‹è¯•")
    print("  python core/main_cuda.py     # è¿è¡Œå®Œæ•´è®­ç»ƒ")


if __name__ == "__main__":
    main()