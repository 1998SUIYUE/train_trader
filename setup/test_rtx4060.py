#!/usr/bin/env python3
"""
RTX 4060 ç¯å¢ƒæµ‹è¯•è„šæœ¬
éªŒè¯CUDAç¯å¢ƒå’Œæ˜¾å¡æ€§èƒ½
"""

import torch
import time
import numpy as np

def test_cuda_environment():
    """æµ‹è¯•CUDAç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥CUDAç¯å¢ƒ...")
    print("-" * 50)
    
    # åŸºæœ¬CUDAæ£€æŸ¥
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        print("è¯·æ£€æŸ¥ï¼š")
        print("  1. NVIDIAé©±åŠ¨æ˜¯å¦å®‰è£…")
        print("  2. CUDA Toolkitæ˜¯å¦å®‰è£…")
        print("  3. PyTorchæ˜¯å¦æ”¯æŒCUDA")
        return False
    
    print("âœ… CUDAå¯ç”¨")
    
    # è®¾å¤‡ä¿¡æ¯
    device_count = torch.cuda.device_count()
    current_device = torch.cuda.current_device()
    device_name = torch.cuda.get_device_name(current_device)
    device_props = torch.cuda.get_device_properties(current_device)
    
    print(f"è®¾å¤‡æ•°é‡: {device_count}")
    print(f"å½“å‰è®¾å¤‡: {current_device}")
    print(f"è®¾å¤‡åç§°: {device_name}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"æ˜¾å­˜æ€»é‡: {device_props.total_memory / 1024**3:.1f} GB")
    print(f"å¤šå¤„ç†å™¨æ•°é‡: {device_props.multi_processor_count}")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºRTX 4060
    if "4060" in device_name:
        print("ğŸ¯ æ£€æµ‹åˆ°RTX 4060æ˜¾å¡ï¼Œé…ç½®ä¼˜åŒ–å»ºè®®ï¼š")
        print("  - æ¨èç§ç¾¤å¤§å°: 1000")
        print("  - æ¨èæ‰¹å¤„ç†å¤§å°: 512")
        print("  - é¢„æœŸæ˜¾å­˜ä½¿ç”¨: 4-6GB")
    
    return True

def test_memory_allocation():
    """æµ‹è¯•æ˜¾å­˜åˆ†é…"""
    print("\nğŸ§ª æµ‹è¯•æ˜¾å­˜åˆ†é…...")
    print("-" * 50)
    
    device = torch.device('cuda')
    
    # æµ‹è¯•ä¸åŒå¤§å°çš„å¼ é‡åˆ†é…
    test_sizes = [
        (1000, 1400),    # å°å‹ç§ç¾¤
        (2000, 1400),    # ä¸­å‹ç§ç¾¤
        (3000, 1400),    # å¤§å‹ç§ç¾¤
    ]
    
    for pop_size, gene_length in test_sizes:
        try:
            # åˆ†é…å¼ é‡
            tensor = torch.randn(pop_size, gene_length, device=device)
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            
            print(f"ç§ç¾¤å¤§å° {pop_size}x{gene_length}: âœ…")
            print(f"  æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB (åˆ†é…) / {cached:.2f}GB (ç¼“å­˜)")
            
            # æ¸…ç†
            del tensor
            torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"ç§ç¾¤å¤§å° {pop_size}x{gene_length}: âŒ æ˜¾å­˜ä¸è¶³")
                break
            else:
                print(f"ç§ç¾¤å¤§å° {pop_size}x{gene_length}: âŒ é”™è¯¯: {e}")

def test_computation_speed():
    """æµ‹è¯•è®¡ç®—é€Ÿåº¦"""
    print("\nâš¡ æµ‹è¯•è®¡ç®—é€Ÿåº¦...")
    print("-" * 50)
    
    device = torch.device('cuda')
    
    # æ¨¡æ‹Ÿé—ä¼ ç®—æ³•çš„çŸ©é˜µè¿ç®—
    pop_size = 1000
    feature_dim = 1400
    n_samples = 1000
    
    print(f"æµ‹è¯•é…ç½®: ç§ç¾¤{pop_size}, ç‰¹å¾{feature_dim}, æ ·æœ¬{n_samples}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    population = torch.randn(pop_size, feature_dim, device=device)
    features = torch.randn(n_samples, feature_dim, device=device)
    
    # æµ‹è¯•çŸ©é˜µä¹˜æ³•é€Ÿåº¦ï¼ˆé€‚åº”åº¦è¯„ä¼°çš„æ ¸å¿ƒè¿ç®—ï¼‰
    num_tests = 10
    times = []
    
    for i in range(num_tests):
        torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
        start_time = time.time()
        
        # æ¨¡æ‹Ÿé€‚åº”åº¦è¯„ä¼°
        scores = torch.mm(population, features.T)  # (pop_size, n_samples)
        fitness = torch.mean(scores, dim=1)        # (pop_size,)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        times.append(end_time - start_time)
    
    avg_time = np.mean(times)
    std_time = np.std(times)
    
    print(f"å¹³å‡è®¡ç®—æ—¶é—´: {avg_time:.4f} Â± {std_time:.4f} ç§’")
    print(f"é¢„ä¼°æ¯ä»£è®­ç»ƒæ—¶é—´: {avg_time * 2:.2f} ç§’")  # è€ƒè™‘å…¶ä»–æ“ä½œçš„å¼€é”€
    
    # æ€§èƒ½è¯„çº§
    if avg_time < 0.1:
        print("ğŸš€ æ€§èƒ½è¯„çº§: ä¼˜ç§€")
    elif avg_time < 0.2:
        print("âœ… æ€§èƒ½è¯„çº§: è‰¯å¥½")
    elif avg_time < 0.5:
        print("âš ï¸  æ€§èƒ½è¯„çº§: ä¸€èˆ¬")
    else:
        print("âŒ æ€§èƒ½è¯„çº§: è¾ƒæ…¢")

def test_mixed_precision():
    """æµ‹è¯•æ··åˆç²¾åº¦æ”¯æŒ"""
    print("\nğŸ”¬ æµ‹è¯•æ··åˆç²¾åº¦æ”¯æŒ...")
    print("-" * 50)
    
    device = torch.device('cuda')
    
    try:
        # æµ‹è¯•FP16æ”¯æŒ
        with torch.cuda.amp.autocast():
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
        
        print("âœ… æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ (FP16)")
        print("  å¯ä»¥ä½¿ç”¨æ··åˆç²¾åº¦å‡å°‘æ˜¾å­˜ä½¿ç”¨")
        
    except Exception as e:
        print(f"âŒ æ··åˆç²¾åº¦æµ‹è¯•å¤±è´¥: {e}")

def recommend_settings():
    """æ¨èé…ç½®è®¾ç½®"""
    print("\nğŸ“‹ RTX 4060 æ¨èé…ç½®...")
    print("-" * 50)
    
    device_name = torch.cuda.get_device_name(0)
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    if "4060" in device_name and total_memory >= 7:  # RTX 4060 8GB
        print("ğŸ¯ RTX 4060 ä¼˜åŒ–é…ç½®:")
        print("""
TRAINING_CONFIG = {
    # RTX 4060 ä¼˜åŒ–å‚æ•°
    "population_size": 1000,        # å……åˆ†åˆ©ç”¨8GBæ˜¾å­˜
    "generations": 100,             # æ¨èè®­ç»ƒä»£æ•°
    "mutation_rate": 0.01,
    "crossover_rate": 0.8,
    "elite_ratio": 0.1,
    
    # æ€§èƒ½ä¼˜åŒ–
    "batch_size": 512,              # å¹³è¡¡é€Ÿåº¦å’Œæ˜¾å­˜
    "checkpoint_interval": 20,      # å®šæœŸä¿å­˜
    "memory_cleanup_interval": 10,  # å®šæœŸæ¸…ç†æ˜¾å­˜
}
        """)
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°RTX 4060ï¼Œä½¿ç”¨é€šç”¨é…ç½®:")
        print("""
TRAINING_CONFIG = {
    "population_size": 500,         # ä¿å®ˆé…ç½®
    "generations": 50,
    "mutation_rate": 0.01,
    "crossover_rate": 0.8,
    "elite_ratio": 0.1,
}
        """)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”§ RTX 4060 ç¯å¢ƒæµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•CUDAç¯å¢ƒ
    if not test_cuda_environment():
        return
    
    # æµ‹è¯•æ˜¾å­˜åˆ†é…
    test_memory_allocation()
    
    # æµ‹è¯•è®¡ç®—é€Ÿåº¦
    test_computation_speed()
    
    # æµ‹è¯•æ··åˆç²¾åº¦
    test_mixed_precision()
    
    # æ¨èé…ç½®
    recommend_settings()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ç¯å¢ƒæµ‹è¯•å®Œæˆï¼")
    print("å¦‚æœæ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è¿è¡Œè®­ç»ƒï¼š")
    print("  cd core")
    print("  python main_cuda.py")

if __name__ == "__main__":
    main()